@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix dice: <https://dice-research.org/> .
@prefix dicepartner: <https://dice-research.org/partner/> .
@prefix dicefunding: <https://dice-research.org/funding/> .
@prefix schema: <https://schema.dice-research.org/> .

dice:LOLA a dice:ProductionReadyProject ;
  schema:tagline "An Open-Source Massively Multilingual Large Language Model" ;
  schema:status "active" ; # current project status
  schema:content """
<p>LOLA is a massively multilingual large language model trained on more than 160 languages using a sparse Mixture-of-Experts Transformer architecture. Our architecture addresses the challenge of harnessing linguistic diversity efficiently while avoiding the common pitfalls of multilinguality. LOLA demonstrates competitive performance in natural language generation and understanding tasks. Its expert-routing mechanism leverages implicit phylogenetic linguistic patterns, potentially alleviating <em>the curse of multilinguality</em>. As an open-source model, LOLA promotes reproducibility and serves as a robust foundation for future research. Our findings pave the way for compute-efficient multilingual models with scalable, strong performance across languages.</p>

<p>To learn more about LOLA, read our preprint: <a href="https://arxiv.org/abs/2409.11272" target="_blank">https://arxiv.org/abs/2409.11272</a></p>

<h3>Resources</h3>
<p>HuggingFace: <a href="https://huggingface.co/dice-research/lola_v1" target="_blank">hf.co/dice-research/lola_v1</a></p>
<p>GitHub: <a href="https://github.com/dice-group/LOLA" target="_blank">github.com/dice-group/LOLA</a></p>
""" ;
#  schema:endDate "2021-07-31"^^xsd:date ;
  schema:startDate "2022-10-01"^^xsd:date ;
  schema:maintainer dice:NikitSrivastava ;
  schema:homepage <https://arxiv.org/abs/2409.11272> ;
  schema:name "LOLA" ;
  schema:logo "lola_logo.png" ;
  schema:sourceCode <https://github.com/dice-group/LOLA> .
